#!/usr/bin/env python3
import os
import re
import time
import sqlite3
import requests
from datetime import datetime
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from fake_useragent import UserAgent
import logging
os.system("caffeinate -s &")  # Starts caffeinate in the background

# --- CONFIGURATION ---
BASE_URL = "https://swisschems.is/shop/"
NUM_PAGES = 9  # pages 1 to 9
IMAGES_FOLDER = "downloaded_images"  # folder to store downloaded images
DB_FILE = "DB/pepsources.db"

# Ensure the images folder exists
if not os.path.exists(IMAGES_FOLDER):
    os.makedirs(IMAGES_FOLDER)

# --- SETUP LOGGING (minimal) ---
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("swisschems_scraper")

# --- SELENIUM CONFIGURATION ---
def configure_selenium():
    ua = UserAgent()
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--ignore-certificate-errors")
    options.add_argument(f"--user-agent={ua.random}")
    
    driver = webdriver.Chrome(options=options)
    driver.implicitly_wait(5)
    driver.execute_cdp_cmd("Page.enable", {})
    driver.execute_cdp_cmd("Page.setBypassCSP", {"enabled": True})
    return driver

# --- DOWNLOAD IMAGE ---
def download_image(image_url: str, product_slug: str) -> str:
    try:
        headers = {"User-Agent": UserAgent().random, "Referer": "https://swisschems.is/"}
        time.sleep(1)
        response = requests.get(image_url, headers=headers, timeout=10)
        if response.status_code != 200:
            logger.info(f"Failed to download image {image_url} (status code {response.status_code})")
            return ""
        filename = f"{product_slug}_{int(time.time())}.webp"
        local_path = os.path.join(IMAGES_FOLDER, filename)
        with open(local_path, "wb") as f:
            f.write(response.content)
        return local_path
    except Exception as e:
        logger.info(f"Error downloading image {image_url}: {e}")
        return ""

# --- DATABASE HELPER: Check if vendor exists ---
def vendor_exists(product_link: str) -> bool:
    try:
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        cursor.execute("SELECT id FROM Vendors WHERE product_link = ?", (product_link,))
        exists = cursor.fetchone() is not None
        conn.close()
        return exists
    except Exception as e:
        logger.info(f"Error checking vendor existence for {product_link}: {e}")
        return False

# --- SCRAPE PRODUCT PAGE DETAILS ---
def scrape_product_page(product_link: str) -> dict:
    driver = configure_selenium()
    driver.get(product_link)
    time.sleep(3)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()
    
    name_elem = soup.find("h1", class_="product_title")
    if not name_elem:
        logger.info(f"Missing title element on {product_link}; skipping.")
        return {}
    product_name = name_elem.get_text(strip=True)
    
    price_elem = soup.find("p", class_="price")
    price_text = ""
    if price_elem:
        ins_elem = price_elem.find("ins")
        if ins_elem:
            price_text = ins_elem.get_text(strip=True)
        else:
            price_text = price_elem.get_text(strip=True)
    
    size = ""
    size_match = re.search(r"\b\d+\s*(mg|ml|IU)\b", product_name, re.IGNORECASE)
    if size_match:
        size = size_match.group(0)
    
    image_elem = soup.find("img", class_="zoomImg")
    product_image_url = ""
    if image_elem:
        product_image_url = image_elem.get("src", "")
        if product_image_url.startswith("//"):
            product_image_url = "https:" + product_image_url
    
    product_slug = re.sub(r"\W+", "_", product_name.lower())
    local_image_path = download_image(product_image_url, product_slug) if product_image_url else ""
    
    return {
        "product_name": product_name,
        "product_link": product_link,
        "product_image": local_image_path,
        "price": price_text,
        "size": size
    }

# --- PARSE A SINGLE PRODUCT ITEM FROM LISTING ---
def parse_product_item(item) -> dict:
    link_elem = item.select_one("a.woocommerce-LoopProduct-link")
    if not link_elem:
        logger.info("Skipping item: missing product link element.")
        return {}
    product_link = link_elem.get("href", "")
    if product_link.startswith("/"):
        product_link = "https://swisschems.is" + product_link

    if vendor_exists(product_link):
        logger.info(f"Product already exists in DB: {product_link}. Skipping.")
        return {}
    
    details = scrape_product_page(product_link)
    if not details:
        logger.info(f"Failed to scrape product page: {product_link}. Skipping.")
        return {}
    
    vendor_dict = {
        "name": "SwissChems",
        "product_name": details.get("product_name", ""),
        "product_link": details.get("product_link", ""),
        "product_image": details.get("product_image", ""),
        "price": details.get("price", ""),
        "size": details.get("size", ""),
        "in_supabase": 0
    }
    return vendor_dict

# --- SCRAPE A SINGLE PAGE ---
def scrape_page(url: str) -> list:
    driver = configure_selenium()
    logger.info(f"Scraping URL: {url}")
    driver.get(url)
    time.sleep(3)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    product_items = soup.select("li.product")
    logger.info(f"Found {len(product_items)} products on page.")
    products = []
    for item in product_items:
        vendor_data = parse_product_item(item)
        if vendor_data:
            products.append(vendor_data)
        else:
            logger.info("Item skipped.")
    return products

# --- INSERT VENDOR INTO DATABASE ---
def insert_vendor(vendor: dict):
    try:
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        cursor.execute("""
            INSERT INTO Vendors (
                name, product_name, product_link, product_image, price, size, in_supabase
            ) VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            vendor.get("name"),
            vendor.get("product_name"),
            vendor.get("product_link"),
            vendor.get("product_image"),
            vendor.get("price"),
            vendor.get("size"),
            vendor.get("in_supabase")
        ))
        conn.commit()
        vendor_id = cursor.lastrowid
        conn.close()
        logger.info(f"Inserted vendor {vendor_id}: '{vendor.get('product_name')}'")
    except Exception as e:
        logger.info(f"Error inserting vendor: {e}")

# --- MAIN PROCESS ---
def main():
    all_products = []
    for page_num in range(1, NUM_PAGES + 1):
        url = BASE_URL if page_num == 1 else f"{BASE_URL}page/{page_num}/"
        logger.info(f"Scraping page {page_num} at URL: {url}")
        products = scrape_page(url)
        logger.info(f"Page {page_num}: {len(products)} new products found.")
        all_products.extend(products)
        time.sleep(3)
    logger.info(f"Total new products scraped from SwissChems: {len(all_products)}")
    
    for vendor in all_products:
        insert_vendor(vendor)
    
    logger.info("Completed scraping and insertion for SwissChems.")

if __name__ == "__main__":
    main()
    os.system("killall caffeinate")  # Stop caffeinate when done